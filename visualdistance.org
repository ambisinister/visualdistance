* Locating Visual Jokes in Homestuck with Rudimentary Computer Vision

#+BEGIN_QUOTE
Draft: v0.2.0 | Posted: x/y/2018 | Updated: 11/19/2018 | confidence of success: 80% | estimated time to completion: 12/28/2018 | importance: Medium
#+END_QUOTE

** Abstract

I use basic computer vision to identify which panels in popular webcomic /[[https://www.homestuck.com/info-story][Homestuck]]/ are visually similar to each other, in order to find examples of reused art / callbacks to other panels. I scrape the webcomic for it's images and apply a number of image distance metrics in order to find one which works. Writeup may contain light spoilers for the webcomic.

** Introduction

In preparation of a much larger project, I came across the following need: I would like to give examples of visually similar images in [[https://en.wikipedia.org/wiki/Andrew_Hussie][Andrew Hussie]]'s webcomic /[[https://www.homestuck.com/story][Homestuck]]/. Hussie reuses drawings frequently, and often repurposes previously drawn panels to easily produce new ones involving different characters (i.e. both characters looking at their hands, from the same angle, using an identical drawing but with one character's skin drawn a different color). Hussie uses this repurposing as a means of introducing visual jokes to his webcomic, frequently calling back to other panels, and then calling back to the callbacks. I'd love to be able to point to a number of these reused panels, but no crowdsourced fanmade database of visually similar images in Homestuck has been created (darn), and since Homestuck has [[http://readmspa.org/stats/][>8000 panels]] combing through these by hand in O(n^2) to dig them up myself sounds like something I would rather not do. 

However, I really wanted those examples of visually similar panels, so I racked my brain about how I could accomplish this until I briefly remembered taking an intro computer vision course in college. The art in Homestuck is pretty simple, so I set out to see what I could do with elementary tools.

If you'd like to play along, the org document for this writeup contains all the code used for this project, and everything used in it can be found <<here>>. I'm going to loosely take a [[https://escholarship.org/uc/item/9050x4r4][reproducible research]] approach here: all the code is written in python and is (hopefully) fairly easy to follow. I think in general [[http://faculty.washington.edu/rjl/lprr.html][Literate Programming]] seems like a bit of a hassle but I figured for a short project it would be a good exercise to try and keep everything organized enough to not feel embarrassed about sharing it.

*** TODO add full environment zip file

** What, Specifically, Do We Want?

As a brief aside, I'll explain what exactly a "win condition" for this project entails. 

Below are two images from two separate panels in Homestuck: 2079 and 2338[fn:1]

[[../images/homestuck/2079_2.gif]]

[[../images/homestuck/2338_1.gif]]

These two images are over 250 pages apart and used for entirely different things, yet they're obvious recolors of the same drawing. This sort of keyframe reuse is really common in Homestuck (and [[https://www.youtube.com/watch?v%3DJU21shbaVBo][media]] in general), but Homestuck's themes of recursion/self-reference and also the sheer magnitude of the webcomic allows for these frames to acquire a sort of meaning unto themselves.

This is nicely illustrated on panel 2488[fn:2]

[[../images/homestuck/2488_1.gif]]

This image is hilarious.

This is a direct callback to the previously reused drawing, despite being an entirely different drawing - the hands are more realistic human hands rather than the stubby hands in the previous two images. The hands are drawn with no border with the ring and little fingers drawn together to give the appearance of four fingers instead of five, just like the older panels.[fn:3]

Homestuck is full of these, and I would like to find as many as possible.

** Assembling the Compendium

Grabbing the non-flash images in /Homestuck/ is straightforward enough. it can be done with relatively few lines of code thanks to our good friends [[http://docs.python-requests.org/en/master/][Requests]] and [[https://www.crummy.com/software/BeautifulSoup/][Beautiful Soup 4]]. The code to grab all the image files in the webcomic can be found below.

#+BEGIN_SRC python
  import requests, bs4
  firstp = 1
  lastp = 10
  #lastp = 8130

  imglist = []

  for x in range(firstp, lastp+1):
      url = "https://www.homestuck.com/story/" + str(x)
      try:
          page = requests.get(url)
          page.raise_for_status()
      except requests.exceptions.RequestException:
          continue                #some numbers are missing from 1-8130, if the link 404s skip it

      soup = bs4.BeautifulSoup(page.text)
      images = soup.find_all('img', class_="mar-x-auto disp-bl")
      
      for count, image in enumerate(images, 1):
          imgurl = image['src']
          if imgurl[0] == '/': imgurl = "https://www.homestuck.com" + imgurl #handle local reference
          response = requests.get(imgurl)
          if response.status_code == 200:
              with open("./screens/img/" + str(x) + "_" + str(count) + "." + imgurl.split(".")[-1], 'wb') as f:
                  f.write(response.content) #format panelnumber_imagecount.format saves all
                
#+END_SRC

This assembles us a corpus of 9,442 images, mostly gifs. This is a pretty decent corpus, as far as datasets for images go, especially considering most images are gifs which contain multiple frames. It's pretty crazy how large this webcomic is, when you have it all in one folder like this. Just the images alone are more than 700MB.

 [[../images/homestuck/dataset.PNG]]

I won't bother with the flashes for now - although they're certainly an important part of the comic and well worth a closer look later, there's well over [[https://www.youtube.com/watch?v%3DAEIOQN3YmNc][three hours]] of flashes and extracting every frame of every flash does not sound fun or necessary for this project for now. 

** Establishing a Baseline with Hamming Distance Of Binary Images

A really basic thing we can start with is taking a black-and-white conversion of the images in the dataset and calculating the [[https://en.wikipedia.org/wiki/Hamming_distance][Hamming Distance]] between them. I have a feeling this won't work particularly well, but it will be useful as a metric of comparison between this and other metrics, plus it should be fairly easy to implement.

We begin with a toy dataset of ten images, which I selected by hand to give a good representative example: The images roughly fall into four groups: [Jade + Robot Jade], [Jade, John, and Terezi at computers], [yellow, green, human hands], [two random images]. Likewise, we will only bother looking at the first frame in these images, despite the fact that they are gifs. As with the flashes, it's not that it would be too difficult to do this (merely splitting the gifs into each frame + instructing the program to ignore frames within the same gif for comparisons would be easy enough), but it would just be a bit more trouble than I think it's worth for now.

Ideally the images in these groups should resemble each other more than they resemble the other images, with the two random images as control. The images that are more direct art recycles should be more similar to each other than they are to merely-similar images (e.g. the images of John and Jade should resemble each other more than they do to Terezi, since John and Jade are in the same spot on the screen and Terezi is translated in the frame).

We can start by converting every image to a binary image consisting of only black and white pixels

#+BEGIN_SRC python
  #Convert all images to binary image
  from PIL import Image
  import os

  for image in os.listdir('./screens/img/'):
      img_orig = Image.open("./screens/img/" + image)
      img_new = img_orig.convert('1')
      dir_save = './screens/binary/' + image
      img_new.save(dir_save)
#+END_SRC

#+RESULTS:
: None

This will allow us to compare each image with a simple pixel-by-pixel comparison and count the number of pixels where the two images differ. While this is very straightforward, it sort of leaves us at the mercy of what colors are used in the panel, so the conversion isn't perfect.

[[../images/homestuck/binary_2079_2.gif]]

[[../images/homestuck/binary_2338_1.gif]]

For example, we have the two hands panels converted to binary images. Here we see that the backgrounds are assigned different colors, as well as the blood being completely eliminated in the first image but not the second. 

There's also some issues with objects blending into the background, which could cause issues as well.

[[../images/homestuck/binary_1033_1.gif]]

This method will likely work extremely well for detecting duplicate images (since they will produce the same binary image) but leave something to be desired for redraws (which have flaws like the two mentioned above).

Anyways, lets give it a shot[fn:6].

#+name: hamming-functions
#+BEGIN_SRC python :results silent
  import PIL
  from PIL import Image
  import io, itertools, os
  from joblib import Parallel, delayed
  import multiprocessing

  def hamming(x, y):
      if len(x) == len(y):
          #Choosing the distance between the image or the image's inverse, whichever is closer
          return min(sum(c1 != c2 for c1, c2 in zip(x, y)), sum(c1 == c2 for c1, c2 in zip(x, y)))
      else:
          return -1

  def compare_img(image1, image2, dire, resize):
      i1 = Image.open(dire + image1)
      #trying to change to 100x100 from 650x450 to see what happens
      if resize: i1 = i1.resize((100,100))
      i1_b = i1.tobytes()
      i2 = Image.open(dire + image2)
      if resize: i2 = i2.resize((100,100))
      i2_b = i2.tobytes()

      dist = hamming(i1_b, i2_b)
      return dist

  def output_format(image1, image2, dire, resize):
      return [image1, image2, compare_img(image1, image2, dire, resize)]

  def hamming_a_directory(dire, resize=True):
      num_cores = multiprocessing.cpu_count()    
      return Parallel(n_jobs=num_cores)(delayed(output_format)(image1, image2, dire, resize)\
                                 for image1, image2 in itertools.combinations(os.listdir(dire), 2))

#+END_SRC
#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  full_list = hamming_a_directory('./screens/binary/')
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:10]
#+END_SRC

#+RESULTS:
| 1525_1.gif | 1525_2.gif | 2179 |
| 2079_2.gif | 2338_1.gif | 2680 |
| 1033_1.gif | 1530_1.gif | 2691 |
| 2488_1.gif | 2079_2.gif | 2695 |
| 1870_1.gif | 1033_1.gif | 2917 |
| 1525_2.gif | 1530_1.gif | 3204 |
| 1034_1.gif | 1525_2.gif | 3240 |
| 1870_1.gif | 1530_1.gif | 3242 |
| 1034_1.gif | 1530_1.gif | 3330 |
| 2338_1.gif | 1530_1.gif | 3539 |

A surprisingly solid baseline! Here we can see that the most similar images with this method are 1525_1 and 1525_2 (John and Jade), which are redraws of each other. Likewise, it catches the similarity between 2079_2 and 2338_1 (the two hands) as well as comparing 2079_2 and 2488_1 (one of the hands + the human gag version).

There are some misses, though -- 1530 is considered similar to 1033 despite the two panels being largely unrelated, which I suspect is largely because of the background for both images being solid black. Likewise, it misses the comparison between 1033_1 and 1034_1, and doesn't compare panels 2338_1 and 2488_1 despite favorably comparing both of those panels to 2079_2. 

So it's clear we can use this to compare images to find similarities, but lets see if we can't get something slightly better.

*** TODO read this? [[https://www.sciencedirect.com/science/article/pii/S1877050915035012][link]]

*** TODO should I put the second comparison on a debug flag? I think it's still N^2 this way but its still (2N)^2 and ehhh not really needed for the non-binary images?

** Edge Detection

[[https://en.wikipedia.org/wiki/Edge_detection][Edge Detection]] is a class of tools in computer vision that mathematically determine points where an image has changes in brightness (i.e. /edges/). This is actually quite a bit more difficult than it seems, since images typically have gradients and non-uniform changes in brightness which make finding the edges in images trickier than it seems. 

That said, the nice thing about line art is that it involves, well, lines, and it seems really probable that edge detection will produce a solid result at extracting the outlines of drawn images. I'm pretty confident that this will yield us some good images so let's try and build it. We will be implementing [[https://en.wikipedia.org/wiki/Canny_edge_detector][Canny edge detection]] which applies a five-step process to the image: 

1. Apply Gaussian Blur (to reduce noise)
2. Find intensity gradients (to find horizontal/vertical/diagonal edges)
3. apply non-maximum suppression (set all parts of the blurred edges to 0 except the local maxima)
4. apply double threshold (split detected edges into "strong", "weak", and "suppressed" based on gradient value)
5. track edges by hysteresis (remove weak edges that aren't near strong edges, usually due to noise)

This is even more straightforward to implement in Python, because [[https://opencv.org/][OpenCV]] / [[https://python-pillow.org/][Pillow]] has built-in support for it already, making this possible without actively writing each step!

#+BEGIN_SRC python
  import cv2 as cv
  import os
  from PIL import Image

  folder = "./screens/img/"
  target = "./screens/canny/"

  for image in os.listdir(folder):
      imgdir = folder + image

      #gif -> png for opencv compatability
      im_gif = Image.open(imgdir)
      saveto = target + image.split(".")[0] + ".png"
      im_gif.save(saveto)

      #Canny Edge Detection, overwrite png
      img_orig = cv.imread(saveto, 1)
      edges = cv.Canny(img_orig,100,200)
      img_new = Image.fromarray(edges)
      img_new.save(saveto)
#+END_SRC

#+RESULTS:
: None

Here's what we end up with:

[[../images/homestuck/edge_2079_2.png]]

[[../images/homestuck/edge_2338_1.png]]

Wow, this turns out great!

We don't get amazing results on every frame, and some of the frames with busier backgrounds suffer a bit from this, like this one:

[[../images/homestuck/edge_1828_2.png]]

But I think the result extracts the edges with enough precision that it's functional enough for now.

#+BEGIN_SRC python :noweb yes exports: strip-export
  <<hamming-functions>>
  full_list = hamming_a_directory('./screens/canny/')
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:10]
#+END_SRC

#+RESULTS:
| 2338_1.png | 2079_2.png |  31 |
| 1033_1.png | 1034_1.png | 224 |
| 2338_1.png | 1033_1.png | 458 |
| 1033_1.png | 2079_2.png | 461 |
| 1870_1.png | 2079_2.png | 479 |
| 2338_1.png | 1870_1.png | 480 |
| 1033_1.png | 1870_1.png | 480 |
| 2338_1.png | 1034_1.png | 514 |
| 2079_2.png | 1034_1.png | 519 |
| 2338_1.png | 2488_1.png | 522 |

The results for this hamming distance are somewhat disappointing: it's really accurate at detecting colorswaps - the hands and the two images of Jade receive appropriately low scores. But it's not so great at detecting reused outlines (the images of Jade and John no longer even crack the top 10 despite being the most similar by binary image hamming distance). 

** Perceptual Hashing

[[https://en.wikipedia.org/wiki/Hash_function][Hash functions]] are functions that can map data of an arbitrary size down to data of a fixed size. Usually these take the form of cryptographic hash functions, which are good for sensitive data because they have high dispersion (they change a lot when the input is changed even a little bit) so its not very useful for working backwards and determining what created the hash. [[http://bertolami.com/index.php?engine%3Dblog&content%3Dposts&detail%3Dperceptual-hashing][Perceptual Hashing]], on the other hand, maps data onto hashes while maintaining a correlation between the source and the hash. If two things are similar, their hashes will be similar with perceptual hashing, which is a useful mechanism for locating similar images (TinEye allegedly uses this for Reverse Image Searching).

[[http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html][Hackerfactor]] has a semi-famous blogpost from 2011 about perceptual hashing algorithms, in which he describes average hashing and pHash - two straightforward and very powerful versions of idea. Average hashing in particular is very easy to grasp: 

1. squish the image down to 8x8 pixels
2. convert to greyscale
3. average colors
4. set every pixel to 1 or 0 depending on whether it is greater/worse than the average
5. turn this binary string into a 64-bit integer. Then, like with our other attempts, you can use hamming distance to compare two images.

Let's give it a whirl.

#+BEGIN_SRC python
  import cv2 as cv
  import os
  import numpy as np
  import PIL
  from PIL import Image

  folder = "./screens/img/"
  target = "./screens/phash/"

  for image in os.listdir(folder):
      imgdir = folder + image

      #resize to 8x8
      im_gif = Image.open(imgdir)
      im_gif = im_gif.resize((8,8))
      saveto = target + image.split(".")[0] + ".png"
      im_gif.save(saveto)

      #convert to greyscale
      im_gif = Image.open(saveto).convert('L')
      im_gif.save(saveto)

      #calculate mean of image
      im_mean = np.mean(list(im_gif.getdata()))

      #for each pixel, assign 0 or 1 if above or below mean
      quantized = []
      for x in list(im_gif.getdata()):
          if x >= im_mean:
              quantized.append(255)
          else:
              quantized.append(0)
      quantized_img = Image.fromarray(np.reshape(quantized, (8,8)).astype('uint8'))
      quantized_img.save(saveto)
#+END_SRC

#+RESULTS:
: None

Just a recap of all the steps:

8x8 image (shown here and also enlarged)

[[../images/homestuck/hands_8x8.png]]
[[../images/homestuck/hands_100x100.png]]

convert to greyscale

[[../images/homestuck/hands_8x8_g.png]]
[[../images/homestuck/hands_100x100_g.png]]

quantize based on mean value

[[../images/homestuck/hands_8x8_q.png]]
[[../images/homestuck/hands_100x100_q.png]]

find hamming distances between images

#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  full_list = hamming_a_directory('./screens/phash/', False)
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:10]
#+END_SRC

#+RESULTS:
| 2338_1.png | 2079_2.png | 17 |
| 1033_1.png | 1870_1.png | 17 |
| 1033_1.png | 1034_1.png | 20 |
| 1525_1.png | 1530_1.png | 23 |
| 1525_2.png | 1530_1.png | 23 |
| 2488_1.png | 2079_2.png | 23 |
| 1525_1.png | 1525_2.png | 24 |
| 2338_1.png | 1033_1.png | 24 |
| 1525_1.png | 2079_2.png | 25 |
| 1525_2.png | 2079_2.png | 25 |

I'm a little unsure what to make of this. On the one hand, it gets almost every single match I wanted. The two hands are the closest, it catches all three of the sitting-at-computer images, it catches the two jades, it seems pretty good. It even seems to be a pretty easy threshold to establish:

#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  import numpy as np
  full_list = hamming_a_directory('./screens/phash/', False)
  full_list.sort(key=lambda x: int(x[2]))
  # mean minus one standard deviation, round up
  return np.ceil(np.mean(zip(*full_list)[2]) - np.std(zip(*full_list)[2]))
#+END_SRC

#+RESULTS:
: 24.0

But I remain perplexed about why 1033 is so insistent on matching up with completely random images. I might try finding hamming distance among both the canny images and the hashes of the images, since the former seems less likely to submit false positives but the latter seems /better/. 

Another variant of this idea is pHash, which uses [[https://en.wikipedia.org/wiki/Discrete_cosine_transform][discrete cosine transform]] (DCT) in place of a simple average. OpenCV has a module for this so I won't bother coding it from scratch.

#+BEGIN_SRC python
  import cv2 as cv
  import os
  import numpy as np
  import PIL
  from PIL import Image

  folder = "./screens/img/"
  target = "./screens/phash/"

  for image in os.listdir(folder):
      imgdir = folder + image

      #gif -> png for opencv compatability
      im_gif = Image.open(imgdir)
      saveto = target + image.split(".")[0] + ".png"
      im_gif.save(saveto)

      #Perceptual Hashing, overwrite png
      img_orig = cv.imread(saveto, 1)
      img_hash = cv.img_hash.pHash(img_orig)[0]
      bin_hash = map(lambda x: bin(x)[2:].rjust(8, '0'), img_hash)

      split_hash = []
      for x in bin_hash:
          row = []
          for y in x:
              row.append(int(y)*255)
          split_hash.append(row)

      img_new = Image.fromarray(np.array(split_hash).astype('uint8'))
      img_new.save(saveto)
#+END_SRC

#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  full_list = hamming_a_directory('./screens/phash/', False)
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:10]
#+END_SRC

#+RESULTS:
| 1525_1.png | 1525_2.png | 17 |
| 2488_1.png | 2079_2.png | 19 |
| 1033_1.png | 1870_1.png | 22 |
| 1530_1.png | 1033_1.png | 23 |
| 2338_1.png | 2488_1.png | 23 |
| 1525_2.png | 2488_1.png | 24 |
| 2488_1.png | 1870_1.png | 24 |
| 1530_1.png | 2488_1.png | 25 |
| 1870_1.png | 2079_2.png | 25 |
| 1525_1.png | 1828_2.png | 26 |

No dice, this is even worse than average hashing.

Alright, as a last ditch attempt, let's try running this on the canny edge-detected images instead of the actual source images.

#+BEGIN_SRC python
  import cv2 as cv
  import os
  import numpy as np
  import PIL
  from PIL import Image

  folder = "./screens/canny/"
  target = "./screens/phash/"

  for image in os.listdir(folder):
      imgdir = folder + image

      #resize to 8x8
      im_gif = Image.open(imgdir)
      im_gif = im_gif.resize((8,8))
      saveto = target + image.split(".")[0] + ".png"
      im_gif.save(saveto)

      #convert to greyscale
      im_gif = Image.open(saveto).convert('L')
      im_gif.save(saveto)

      #calculate mean of image
      im_mean = np.mean(list(im_gif.getdata()))

      #for each pixel, assign 0 or 1 if above or below mean
      quantized = []
      for x in list(im_gif.getdata()):
          if x >= im_mean:
              quantized.append(255)
          else:
              quantized.append(0)
      quantized_img = Image.fromarray(np.reshape(quantized, (8,8)).astype('uint8'))
      quantized_img.save(saveto)
#+END_SRC

#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  import numpy as np
  full_list = hamming_a_directory('./screens/phash/', False)
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:10]
#+END_SRC

#+RESULTS:
| 2338_1.png | 2079_2.png | 0 |
| 1525_1.png | 1530_1.png | 2 |
| 1525_1.png | 2338_1.png | 2 |
| 1525_1.png | 1870_1.png | 2 |
| 1525_1.png | 2079_2.png | 2 |
| 1530_1.png | 2338_1.png | 2 |
| 1530_1.png | 1870_1.png | 2 |
| 1530_1.png | 2079_2.png | 2 |
| 2338_1.png | 1870_1.png | 2 |
| 2338_1.png | 1034_1.png | 2 |

Again, no dice; all of the images are far too similar to create substantially different hashes, which means the list of false matches is extraordinarily high.

** Clustering

We take a brief pause here to ponder the following question: how are we going to pull out clusters of related images in a sea of comparisons? It's a bit weird of a problem, since there's no validation set, an unknown number of clusters, and an undefined/large quantity of "clusters" with cluster size 1 (i.e. unique panels). 

The solution I think I'm going to take here is a very very simple one, keeping with the general idea of this being a relatively beginner take on the problem. We're going to take a two-step approach to pulling out the clusters.

*** Pruning

First, we are going to filter the images by ones that appear to be present in at least one cluster. Doing this is pretty straightforward - we can just calculate the mean and standard deviation of each panel and filter out images that are sufficiently far away from the average panel. This will allow us to only cluster data that actually can be clustered meaningfully, since after doing this we can just ignore unique panels.

Something I'll note about my implementation below is that I first prune outliers that are too far in the wrong direction -- i.e. images that are too different from the panel in question. There's no effective difference between the average panel and an unusually distant panel, since both of them are considered non-match in this procedure, but those outliers can sometimes pull the mean / standard deviation a bit higher than is useful. In the below implementation I recalculate the mean among "average" panels and matches, which results in a lower mean with a smaller standard deviation, and then use that to determine matches.

#+name: filter_dupes
#+BEGIN_SRC python :noweb yes exports: strip-export
  import numpy as np
  import os
  from PIL import Image
  <<hamming-functions>>

  def nxn_grid_from_itertools_combo(panels, full_list):
      # Create nxn grid such that x,y is a comparison between panel x and panel y
      #   this is the format that you'd get if you did every comparison but we used itertools
      #   be more efficient. Now that we need these comparisons in a matrix we need to convert it.
      grid = []
      
      for image1 in panels:
          compare_img1 = []
          for image2 in panels:
              if image1 == image2:
                  compare_img1.append(0)
              else:
                  val = [x[2] for x in full_list if ((x[0] == image1 and x[1] == image2) or \
                                              (x[0] == image2 and x[1] == image2))]
                  if val: compare_img1.append(val[0])
                  else: compare_img1.append(grid[panels.index(image2)][panels.index(image1)])
              
          grid.append(compare_img1)

      return grid
      

  def filter_out_nonduplicates(directory, resize=True):
      ## Perform comparisons without duplicates

      full_list = hamming_a_directory(directory, resize)

      ## convert comparisons to an nxn grid, as if we had duplicates

      # Create list of panels
      panels = []

      for image in os.listdir(directory):
          panels.append(image)

      # Create nxn grid such that x,y is a comparison between panel x and panel y
      nxn_grid = nxn_grid_from_itertools_combo(panels, full_list)

      # find mu and sigma of each panel compared to each other panel, filter out probable matches
      likely_a_cluster = []

      for x in nxn_grid:
          matches = []
      
          #calculate raw
          avg_a = np.mean(x)
          stdv_a = np.std(x)
          vals = []

          #remove extremely different images to recalibrate baseline (wrong end outliers)
          for y in x:
              if y < avg_a + 2*stdv_a: vals.append(y)

          #calculate among more similar images
          if not resize: vals.remove(0)
          avg_b = np.mean(vals)
          stdv_b = np.std(vals)

          #return 1 if more than a standard deviation below average, 0 otherwise
          for y in x:
              if y > avg_b - stdv_b:
                  matches.append(0)
              else: matches.append(1)

          likely_a_cluster.append(matches)

      # return list of images to use
      match_image_names = []

      for i, assessment in enumerate(likely_a_cluster):
          if sum(assessment) > 1: match_image_names.append(panels[i])

      if not match_image_names: return likely_a_cluster

      # also return pruned nxc grid where c is the length of match_image_names
      pruned_grid = []

      for i, picture in enumerate(match_image_names):
          pruned_grid.append(nxn_grid[panels.index(picture)])

      # return format is [pruned names, full nxn grid, pruned nxc grid]
      return [match_image_names, nxn_grid, pruned_grid]

  def move_directory(directory, filename):
      if not os.path.exists(directory+"filtered/"): os.mkdir(directory+"filtered/")
      try:
          newfile = Image.open(directory+filename)
          newfile.save(directory+"filtered/"+filename)
      except:        
          newfile = Image.open(directory+filename[:-3]+"gif")
          newfile.save(directory+"filtered/"+filename[:-3]+"gif")
#+END_SRC
#+BEGIN_SRC python :noweb yes exports: strip-export 
<<filter_dupes>>

return filter_out_nonduplicates('./screens/canny/')[0]
#+END_SRC

#+RESULTS:
| 2338_1.png | 1033_1.png | 2079_2.png | 1034_1.png |

*** K-Means Clustering Applied to Comparing N Images

Then, we can apply a variation on [[https://en.wikipedia.org/wiki/K-means_clustering][k-means clustering]] to pull apart these values. This is probably not the most efficient way to do it, but it's pretty cool! 

**** A Brief overview of K-Means Clustering

K-means clustering works via a four step process:

1. Initialize k random points in n-dimensional space, usually points in the dataset
2. Group all data points to the closest point
3. When all points are grouped, calculate the mean of everything assigned to that point
4. If the grouping of points changed, repeat step 2 with the new mean in place of the old K. If they stayed the same, return the clustering and stop.

[[../images/homestuck/kmeans.gif]][fn:4]

This is intuitive for clustering things relative to variables, but it’s not immediately obvious how we can apply it to our images.

To illustrate it, imagine a 2D plane with the x-axis representing “distance to panel A” and the y-axis representing “distance to panel B”

[[../images/homestuck/2_axis_nocompare.png]]

So if we take any random panel and use the hamming distance, you can represent this image in the “space” of these two panels. Proximity to 0 represents similarity, distance from 0 represents dissimilarity. So using panel A would yield something like (0, 15000) since panel A == itself, and likewise using panel B would yield something like (15000, 0). If you introduced panel C, which is a redraw of panel A, you might expect a value like (800, 15000). If we were only trying to cluster our images based on these two panels, the k-means solution makes perfect sense.

[[../images/homestuck/2_axis_onecompare.png]]

So you can imagine a third panel being considered as a z-axis, which turns this into a 3d space. It’s in three dimensions now, but the basic idea is still the same, and k-means solution still makes sense (just using three random values per point instead of 2).

[[../images/homestuck/3axes.png]]

We extend this from 3-dimensional space to n-dimensional space, which is harder to represent visually but is the same structurally as before — you can represent an image by its distance to every other image in the set, and you can initialize a point in this n-dimensional space by generating a list of n random numbers: [distance from panel_1, distance from panel_2, distance from panel_3, ... , distance from panel_n ].

We can increment /k/ starting from 1, and we can run each value of /k/ a few times and pick the lowest variation clusters. We can loosely adapt the [[https://en.wikipedia.org/wiki/Elbow_method_(clustering)][elbow method]] to select a value of K.[fn:5]

Its using this framework in which we can apply k-means clustering as an ok means of sorting the images into visually similar groups.

***** TODO wouldn't it just make more sense to use the actual pixel values for the images? hamming distance to each panel works reasonably well but if we're using 8x8 hashes then I can just cluster based on R^64 instead of R^9000whatever

**** Implementation

#+name: clustering
#+BEGIN_SRC python :noweb yes exports: strip-export
  import random, math
  import numpy as np
  from joblib import Parallel, delayed
  import multiprocessing

  def find_distance(x,y):
      #sqrt ( (a-x)^2 + (b-y)^2 + (c-z)^2 ... )
      distance = 0
      for origin, destination in zip(x,y):
          distance += (origin-destination)**2
      
      return math.sqrt(distance)

  def kmeans_clustering(matrix, k):
      #init k random points
      points = random.sample(matrix, k)

      #placeholders
      oldclusters = [-1]
      clusters = []
      for i in range(k): clusters.append([])
      emptyclusters = clusters

      #loop until points don't change
      while(oldclusters != clusters):
          clusters = emptyclusters #use space instead of time to avoid iterating to zero out every loop

          #group all data points to nearest point
          for x in matrix:
              distances = []
              
              for y in points:
                  distances.append(find_distance(x,y))

              clusters[distances.index(min(distances))].append(x)

          #when all points are grouped, calculate new mean for each point
          for i, cluster in enumerate(clusters):
              if cluster:
                  points[i] = map(np.mean, zip(*cluster))

          oldclusters = clusters

      return clusters

  def cluster_given_K(matrix, k, n=50):
      # run K-means a few times, return clustering with minimum intracluster variance
      clusterings = []

      # run k-means a few times
      for x in range(n):
          clusterings.append(kmeans_clustering(matrix, k))

      # calculate intracluster variance for each clustering
      ##  this is just the sum of all distances from every point to it's cluster's center
      distances = []
      for clustering in clusterings:
          variance = 0
          for cluster in clustering:
              center = map(np.mean, zip(*cluster))
              for point in cluster:
                  variance += find_distance(point,center)

          distances.append(variance)
          
      return [clusterings[distances.index(min(distances))], min(distances)]

  def elbowmethod(candidates, debug_flag):
      varscores = zip(*candidates)[1]
      if debug_flag == 1: return varscores
      percentages = map(lambda x: 1-(x/varscores[0]), varscores)

      elbowseek = []

      for point in range(0,len(percentages)-1):
          if point is 0: elbowseek.append(0)
          elif point is len(percentages)-1: elbowseek.append(percentages[point]-percentages[point-1])
          else: elbowseek.append((percentages[point]-percentages[point-1]) - \
                                 (percentages[point+1]-percentages[point]))

      return elbowseek

  def cluster(matrix, minK=1, maxK=-1, runs=50, debug_flag=0):
      if not matrix: return []
      if maxK is -1: maxK = len(matrix)

      num_cores = multiprocessing.cpu_count()
      candidates = Parallel(n_jobs=num_cores)(delayed(cluster_given_K)(matrix, x, runs) for x in range(minK, maxK))
      
      elbowseek = elbowmethod(candidates, debug_flag)

      if debug_flag == 1: return elbowseek, candidates
      
      return candidates[elbowseek.index(max(elbowseek))][0]

  def give_names(clustering, names, grid):
      ret = []

      for x in clustering:
          ret_a = []
          for y in x:
              ret_a.append(names[grid.index(y)])
          ret.append(ret_a)

      return ret
#+END_SRC
#+BEGIN_SRC python :noweb yes exports: strip-export 
<<clustering>>
return cluster([[1,1], [1,1], [1,0], [1,3], [10,12], [10,11], [10,10], [20,20], [22,20], [21,21]])
#+END_SRC

#+RESULTS:
| (1 1)   | (1 1)   | (1 0)   | (1 3) |
| (20 20) | (22 20) | (21 21) |       |
| (10 12) | (10 11) | (10 10) |       |

Awesome, we have an implementation working now. 

**** K-Means vs Canny Edge Detection

Just so we can see before running it on the full webcomic pruned to only include likely-clustered images, lets just see what we get if we run it on our 10 image dataset of canny edge-detected images.

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/canny/'
  full_list = hamming_a_directory(directory)
  panels = []
  for image in os.listdir(directory): panels.append(image)
  grid = nxn_grid_from_itertools_combo(panels, full_list)

  return give_names(cluster(grid), panels, grid)
#+END_SRC

#+RESULTS:
| 1525_1.png | 1525_2.png | 1530_1.png | 2338_1.png | 2488_1.png | 1033_1.png | 1870_1.png | 2079_2.png | 1034_1.png |
| 1828_2.png |            |            |            |            |            |            |            |            |

Well, that's sort of funny; the elbow method yields k=2 here because 1828_2 is so noisy compared to all the other panels, which certainly makes enough sense. Let's see if we can force it to use at least three clusters.

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/canny/'
  full_list = hamming_a_directory(directory)
  panels = []
  for image in os.listdir(directory): panels.append(image)
  grid = nxn_grid_from_itertools_combo(panels, full_list)

  return give_names(cluster(grid, 3), panels, grid)
#+END_SRC

#+RESULTS:
| 2338_1.png | 2079_2.png |
| 1033_1.png | 1034_1.png |
| 1870_1.png |            |
| 1530_1.png |            |
| 1525_2.png |            |
| 1525_1.png |            |
| 2488_1.png |            |
| 1828_2.png |            |

That's better.

One thing I'm noticing here is that there's some pretty big random variation here, and that running each cluster multiple times is very helpful for zeroing in on a good result. 

Let's run it on the pruned list real fast just to make sure the implementation works the full way through.

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/canny/'
  ham = filter_out_nonduplicates(directory)
  return give_names(cluster(ham[2]), ham[0], ham[2])
#+END_SRC

#+RESULTS:
| 1033_1.png | 1034_1.png |
| 2338_1.png | 2079_2.png |

**** K-Means vs Perceptual Hashes of Images

Something funny I'm noticing is that the elbow method fails terribly for the hash images, but it's pretty solid if you have a value for K determined already. Here's what it wants to spit out normally:

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/phash/'
  ham = filter_out_nonduplicates(directory, False)
  return give_names(cluster(ham[2]), ham[0], ham[2])
#+END_SRC

#+RESULTS:
| 1525_1.png | 1525_2.png | 1530_1.png | 2338_1.png | 2488_1.png | 2079_2.png | 1828_2.png |
| 1033_1.png | 1870_1.png | 1034_1.png |            |            |            |            |

Yuck! Here's the same code but with a narrow range of k-values already selected:

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>
  import matplotlib.pyplot as plt

  directory = './screens/phash/'
  ham = filter_out_nonduplicates(directory, False)
  return give_names(cluster(ham[2], 4, 6, 100), ham[0], ham[2])
#+END_SRC

#+RESULTS:
| 1828_2.png |            |            |
| 1525_1.png | 1525_2.png | 1530_1.png |
| 1033_1.png | 1870_1.png | 1034_1.png |
| 2338_1.png | 2488_1.png | 2079_2.png |

It's perfect aside from the miss on 1870 discussed earlier. A huge improvement compared to the same values of K applied to merely the canny images, which we can visualize below

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  import matplotlib.pyplot as plt

  directory = './screens/phash/'
  ham = filter_out_nonduplicates(directory, False)
  clust = cluster(ham[2], 1, -1, 100, 1)[0]
  plt.plot(range(1, len(clust)+1), clust)
  plt.title('SSE vs K - Image Hashes')
  plt.xlabel('Value of K')
  plt.ylabel('SSE')
  plt.show()

  directory = './screens/canny/'
  full_list = hamming_a_directory(directory)
  panels = []
  for image in os.listdir(directory): panels.append(image)
  grid = nxn_grid_from_itertools_combo(panels, full_list)
  clust = cluster(grid, 1, -1, 100, 1)[0]
  plt.plot(range(1, len(clust)+1), clust)
  plt.title('SSE vs K - Edge Detected Images')
  plt.xlabel('Value of K')
  plt.ylabel('SSE')
  plt.show()

  return give_names(cluster(grid, 4, 6, 100), panels, grid)
#+END_SRC

#+RESULTS:
| 1828_2.png |            |            |            |            |            |
| 1525_1.png |            |            |            |            |            |
| 1525_2.png | 1530_1.png | 2488_1.png | 1033_1.png | 1870_1.png | 1034_1.png |
| 2338_1.png | 2079_2.png |            |            |            |            |

The elbow method is completely failing us, and I don't blame it, given the following results for the variances by value of K:

[[../images/homestuck/elbow_hash.png]]

[[../images/homestuck/elbow_edge.png]]

It worked fine for the edge detection case (in this case, the first try with 1828_2 separated out) but there's no elbow in sight for the image hashes. I don't know if this will extend to the full dataset or not, but I'll have to rethink things if that ends up being the case.

** Giving it a whirl 

Clustering aside, let's just see what happens if we find the hamming distances on the hashes:


#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  import numpy as np
  full_list = hamming_a_directory('./screens/phash/', False)
  full_list.sort(key=lambda x: int(x[2]))
  return full_list[:20]
#+END_SRC

#+RESULTS:
| 1005_1.png | 1008_1.png | 0 |
| 1006_2.png | 1016_1.png | 0 |
| 1012_1.png | 1015_1.png | 0 |
| 1020_2.png | 1020_3.png | 0 |
| 1020_2.png | 1120_1.png | 0 |
| 1020_2.png | 1121_1.png | 0 |
| 1020_2.png | 1125_1.png | 0 |
| 1020_2.png | 1468_1.png | 0 |
| 1020_2.png | 7546_1.png | 0 |
| 1020_3.png | 1120_1.png | 0 |
| 1020_3.png | 1121_1.png | 0 |
| 1020_3.png | 1125_1.png | 0 |
| 1020_3.png | 1468_1.png | 0 |
| 1020_3.png | 7546_1.png | 0 |
| 1034_2.png | 1079_2.png | 0 |
| 1038_2.png | 1117_2.png | 0 |
| 1053_2.png | 1054_1.png | 0 |
| 1053_2.png | 1055_1.png | 0 |
| 1053_2.png | 1056_1.png | 0 |
| 1053_2.png | 1810_1.png | 0 |

Right away the results are super interesting; and just seeing which images generate the same hash makes me fairly optimistic about our ability to cluster images. For example, from the looks of this images containing Alchemiter Recipes are all being thrown into the same hash, which is remarkably encouraging. 

Let's get all the images that we are at least semi-confident could belong to clusters.

#+BEGIN_SRC python :noweb yes :exports strip-export
  <<hamming-functions>>
  <<filter_dupes>>
  import numpy as np
  full_list = hamming_a_directory('./screens/phash/', False)
  # mean minus one standard deviation, round up
  threshold = np.ceil(np.mean(zip(*full_list)[2]) - np.std(zip(*full_list)[2]))

  matches = []
  for x in full_list:
      if x[2] <= threshold:
          if x[0] not in matches: matches.append(x[0])
          if x[1] not in matches: matches.append(x[1])
  for x in matches: move_directory("./screens/img/", x)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/canny/'
  ham = filter_out_nonduplicates(directory)
  for x in ham[0]: move_directory("./screens/canny/", x)

#+END_SRC


Please write code that builds directories and places matches into them.

FWIW I need to add sylladex jokes into this writeup 10000%

used: 

#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>

  directory = './screens/canny/'
  full_list = hamming_a_directory(directory)
  panels = []
  for image in os.listdir(directory): panels.append(image)
  grid = nxn_grid_from_itertools_combo(panels, full_list)

  return give_names(cluster(grid, 3), panels, grid)
#+END_SRC

#+RESULTS:
| 1525_2.png |            |
| 1033_1.png | 1034_1.png |
| 2488_1.png |            |
| 1828_2.png |            |
| 1870_1.png |            |
| 1525_1.png |            |
| 2338_1.png | 2079_2.png |
| 1530_1.png |            |


#+BEGIN_SRC python :noweb yes exports: strip-export 
  <<clustering>>
  <<filter_dupes>>
  import matplotlib.pyplot as plt

  directory = './screens/phash/'
  ham = filter_out_nonduplicates(directory, False)
  return give_names(cluster(ham[2], 4, 6, 100), ham[0], ham[2])
#+END_SRC

#+RESULTS:
| 1828_2.png |            |            |
| 1033_1.png | 1870_1.png | 1034_1.png |
| 1525_1.png | 1525_2.png | 1530_1.png |
| 2338_1.png | 2488_1.png | 2079_2.png |



* Footnotes

[fn:1] https://www.homestuck.com/story/2079 

https://www.homestuck.com/story/2338

[fn:2] https://www.homestuck.com/story/2488

[fn:3] The dialogue in the human-hands panel is a callback to one of the earlier panels as well.

#+begin_QUOTE
Look at this mess. All this blood and sunlight is stirring bright feelings within.

-2338
#+END_QUOTE

#+BEGIN_QUOTE
Ugh. This troll paint is making a mess. This was such a bad idea.

-2488
#+END_QUOTE

[fn:4] excellent visualization gif taken from http://shabal.in/visuals/kmeans/2.html

[fn:5] The elbow method is a mostly visual method which /really/ bothers me given that a lot of times it merely reduces the problem from "visually group points into clusters" to "visually select a value of K which forms an elbow". I do some basic arithmetic to emulate the elbow method empirically but if it doesn't work with the larger dataset then I'll just use gap statistic or something.

[fn:6] Note that here and in the K-Means clustering I have some code in here that does some straightforward parallelization to use all the cores instead of one. This doesn't really do anything for the toy dataset but I'm hoping it'll speed up the process substantially during the full comic runs.



